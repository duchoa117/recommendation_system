#import library
from pyvi import ViTokenizer, ViPosTagger
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
import re
import time
import pandas as pd
#___________________________________________________
#import module
from connect_mongodb import *
from recommendation_system import RecommendationSys, utils
#___________________________________________________
#statics variables
PRODUCTS = None
VECTORIZER = TfidfVectorizer()
TEXTMATRIX = None

#___________________________________________________
#code
class ContentBased(RecommendationSys):
    @staticmethod
    def normalize_text(text):
        # link
        text = re.sub(r"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+", ' <link> ', text, flags=re.MULTILINE)

        # email
        text = re.sub(r"[\w\.-]+@[\w\.-]+", ' <email> ', text, flags=re.MULTILINE)

        # image
        text = re.sub(r"(\w)*\.(?:jpg|gif|png)", ' <image> ', text, flags=re.MULTILINE)

        # atm       
        text = re.sub(r"\b\d{12,16}\b", ' <atm> ', text, flags=re.MULTILINE)

        # phone
        text = re.sub(r"\b\d{10,11}\b", ' <telephone> ', text, flags=re.MULTILINE)

        # price
        text = re.sub(r"\d+(k|l|K|L|\$|USD)", ' <money> ', text, flags=re.MULTILINE)

        # measure
        text = re.sub(r"\d+(cm|m|km|mm|g|kg)", ' <measure> ', text, flags=re.MULTILINE)

        # unusual
        text = re.sub(r"(/|\|\n\n|\n)", ' ', text, flags=re.MULTILINE)

        # emoji
        text = re.sub(
            u"([\U00002600-\U000027BF])|([\U0001f300-\U0001f64F])|([\U0001f680-\U0001f6FF])|([\U00010000-\U0010ffff])",
            ' <emoji> ', text, flags=re.MULTILINE)

        # Remove c√°c k√Ω t·ª± k√©o d√†i: vd: ƒë·∫πppppppp
        text = re.sub(r'([A-Z])\1+', lambda m: m.group(1).upper(), text, flags=re.IGNORECASE)

        # Chuy·ªÉn th√†nh ch·ªØ th∆∞·ªùng
        text = text.lower()

        replace_list = {
            '√≤a': 'o√†', '√≥a': 'o√°', '·ªèa': 'o·∫£', '√µa': 'o√£', '·ªça': 'o·∫°', '√≤e': 'o√®', '√≥e': 'o√©', '·ªèe': 'o·∫ª',
            '√µe': 'o·∫Ω', '·ªçe': 'o·∫π', '√πy': 'u·ª≥', '√∫y': 'u√Ω', '·ªßy': 'u·ª∑', '≈©y': 'u·ªπ', '·ª•y': 'u·ªµ', 'u·∫£': '·ªßa',
            'aÃâ': '·∫£', '√¥ÃÅ': '·ªë', 'u¬¥': '·ªë', '√¥ÃÉ': '·ªó', '√¥ÃÄ': '·ªì', '√¥Ãâ': '·ªï', '√¢ÃÅ': '·∫•', '√¢ÃÉ': '·∫´', '√¢Ãâ': '·∫©',
            '√¢ÃÄ': '·∫ß', 'oÃâ': '·ªè', '√™ÃÄ': '·ªÅ', '√™ÃÉ': '·ªÖ', 'ƒÉÃÅ': '·∫Ø', 'uÃâ': '·ªß', '√™ÃÅ': '·∫ø', '∆°Ãâ': '·ªü', 'iÃâ': '·ªâ',
            'eÃâ': '·∫ª', '√†k': u' √† ', 'aÀã': '√†', 'iÀã': '√¨', 'ƒÉ¬¥': '·∫Ø', '∆∞Ãâ': '·ª≠', 'eÀú': '·∫Ω', 'yÀú': '·ªπ', 'a¬¥': '√°',

            # Chu·∫©n h√≥a 1 s·ªë sentiment words/English words
            '√¥ k√™i': ' ok ', 'okie': ' ok ', ' o k√™ ': ' ok ',
            'okey': ' ok ', '√¥k√™': ' ok ', 'oki': ' ok ', ' oke ': ' ok ', ' okay': ' ok ', 'ok√™': ' ok ',
            ' tks ': u' c√°m ∆°n ', 'thks': u' c√°m ∆°n ', 'thanks': u' c√°m ∆°n ', 'ths': u' c√°m ∆°n ', 'thank': u' c√°m ∆°n ',
            '‚≠ê': 'star ', '*': 'star ', 'üåü': 'star ', u'\n': ' ', u')': ' ', u'(': ' ',
            'kg ': u' kh√¥ng ', 'not': u' kh√¥ng ', u' kg ': u' kh√¥ng ', '"k ': u' kh√¥ng ', ' kh ': u' kh√¥ng ',
            'k√¥': u' kh√¥ng ', 'hok': u' kh√¥ng ', ' kp ': u' kh√¥ng ph·∫£i ', u' k√¥ ': u' kh√¥ng ', '"ko ': u' kh√¥ng ',
            u' ko ': u' kh√¥ng ', u' k ': u' kh√¥ng ', 'khong': u' kh√¥ng ', u' hok ': u' kh√¥ng ',
            ' vs ': u' v·ªõi ', 'wa': ' qu√° ', 'w√°': u' qu√°', 'j': u' g√¨ ', '‚Äú': ' ',
            ' sz ': u' c·ª° ', 'size': u' c·ª° ', u' ƒëx ': u' ƒë∆∞·ª£c ', 'dk': u' ƒë∆∞·ª£c ', 'dc': u' ƒë∆∞·ª£c ', 'ƒëk': u' ƒë∆∞·ª£c ',
            'ƒëc': u' ƒë∆∞·ª£c ', 'authentic': u' chu·∫©n ch√≠nh h√£ng ', u' aut ': u' chu·∫©n ch√≠nh h√£ng ',
            u' auth ': u' chu·∫©n ch√≠nh h√£ng ', 'thick': u' positive ', 'store': u' c·ª≠a h√†ng ',
            'shop': u' c·ª≠a h√†ng ', 'sp': u' s·∫£n ph·∫©m ', 'gud': u' t·ªët ', 'god': u' t·ªët ', 'wel done': ' t·ªët ',
            'good': u' t·ªët ', 'g√∫t': u' t·ªët ',
            's·∫•u': u' x·∫•u ', 'gut': u' t·ªët ', u' tot ': u' t·ªët ', u' nice ': u' t·ªët ', 'perfect': 'r·∫•t t·ªët',
            'bt': u' b√¨nh th∆∞·ªùng ',
            'time': u' th·ªùi gian ', 'q√°': u' qu√° ', u' ship ': u' giao h√†ng ', u' m ': u' m√¨nh ', u' mik ': u' m√¨nh ',
            '√™Ãâ': '·ªÉ', 'product': 's·∫£n ph·∫©m', 'quality': 'ch·∫•t l∆∞·ª£ng', 'chat': ' ch·∫•t ', 'excelent': 'ho√†n h·∫£o',
            'bad': 't·ªá', 'fresh': ' t∆∞∆°i ', 'sad': ' t·ªá ',
            'date': u' h·∫°n s·ª≠ d·ª•ng ', 'hsd': u' h·∫°n s·ª≠ d·ª•ng ', 'quickly': u' nhanh ', 'quick': u' nhanh ',
            'fast': u' nhanh ', 'delivery': u' giao h√†ng ', u' s√≠p ': u' giao h√†ng ',
            'beautiful': u' ƒë·∫πp tuy·ªát v·ªùi ', u' r ': u' r·ªìi ', u' shopE ': u' c·ª≠a h√†ng ', u' order ': u' ƒë·∫∑t h√†ng ',
            'ch·∫•t lg': u' ch·∫•t l∆∞·ª£ng ', u' sd ': u' s·ª≠ d·ª•ng ', u' dt ': u' ƒëi·ªán tho·∫°i ', u' nt ': u' nh·∫Øn tin ',
            u' tl ': u' tr·∫£ l·ªùi ', u' s√†i ': u' x√†i ', u'bjo': u' bao gi·ªù ',
            'thik': u' th√≠ch ', u' sop ': u' c·ª≠a h√†ng ', ' fb ': ' facebook ', ' face ': ' facebook ', ' very ': u' r·∫•t ',
            u'qu·∫£ ng ': u' qu·∫£ng  ',
            'dep': u' ƒë·∫πp ', u' xau ': u' x·∫•u ', 'delicious': u' ngon ', u'h√†g': u' h√†ng ', u'q·ªßa': u' qu·∫£ ',
            'iu': u' y√™u ', 'fake': u' gi·∫£ m·∫°o ', 'trl': 'tr·∫£ l·ªùi', '><': u' positive ',
            ' por ': u' t·ªá ', ' poor ': u' t·ªá ', 'ib': u' nh·∫Øn tin ', 'rep': u' tr·∫£ l·ªùi ', u'fback': ' feedback ',
            'fedback': ' feedback ',

            # c√°c t·ª´ nghi v·∫•n
            'nh·ªâ': ' <endturn> ', '?': ' <endturn> ', 'th·∫ø n√†o': ' <endturn> ', 'c√≤n kh√¥ng': ' <endturn> ',
            'c√≥ kh√¥ng': ' <endturn> ', 'c√≤n ko': ' <endturn> ', 'c√≥ ko': ' <endturn> ',
        }

        for k, v in replace_list.items():
            text = text.replace(k, v)
    #     # chuyen punctuation th√†nh space
    #     translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))
    #     text = text.translate(translator)
        # remove n·ªët nh·ªØng k√Ω t·ª± th·ª´a th√£i
        text = text.replace(u'"', u' ')
        text = text.replace(u'Ô∏è', u'')
        text = text.replace('üèª', '')
        text = text.strip()
        text = ViTokenizer.tokenize(text)
        return text
    @staticmethod 
    def pull_des():
        global PRODUCTS
        PRODUCTS = pd.DataFrame(list(products.find({}, {"product_id", "description"}))).drop(columns="_id")
    @staticmethod
    def update_des():
        pass
    @staticmethod
    def normalize_des():
        (m, n) = PRODUCTS.shape
        for i in range(m):
            PRODUCTS.loc[i, "description"] = ContentBased.normalize_text(
                                                PRODUCTS.loc[i, "description"])
    @staticmethod
    def vectorizer_text_model():
        global VECTORIZER
        global PRODUCTS
        VECTORIZER.fit(PRODUCTS['description'])
    @staticmethod
    def text_to_vector():
        global TEXTMATRIX
        global VECTORIZER
        global PRODUCTS
        TEXTMATRIX =  VECTORIZER.transform(PRODUCTS['description'])
    @staticmethod
    def gennerate_recommend(product_id, user_trans = []):
        cosine_sim = cosine_similarity(TEXTMATRIX)
        indices = pd.Series(PRODUCTS.loc[:, 'product_id'], )
        recommended_products = []
        idx = indices[indices == product_id].index[0]
        score_series = pd.Series(cosine_sim[idx]).sort_values(ascending = False)
        top_3_indexes = list(score_series.iloc[1:4].index)
        temp = list(PRODUCTS.loc[score_series.iloc[1:4].index, 'product_id'])
        count = 0
        for t in temp:
            if(t not in user_trans):
                recommended_products.append(t)
                count += 1
                if(count > 3): break
        return sample(recommended_products,2)
    @staticmethod
    def cold_start():
        ContentBased.pull_des()    
        ContentBased.normalize_des()    
        ContentBased.vectorizer_text_model()
        ContentBased.text_to_vector()
if __name__ == "__main__":
    #start
    start_time = time.time()
    #______________________________________________________________________________________

    ContentBased.pull_des()    
    ContentBased.normalize_des()    
    ContentBased.vectorizer_text_model()
    ContentBased.text_to_vector()
    t = ContentBased.gennerate_recommend("1")
    print(t)
    #end
    print(" Execute time: %s seconds" % (time.time() - start_time))
    #______________________________________________________________________________________




